<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
"http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
	<head>
		<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
		<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
		<link rel="stylesheet" href="./jemdoc.css" type="text/css" />
		<link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto">
		<title>Jonghwan Mun</title>
	</head>
   <body>
      <div id="layout-content">
         <p><br />
         </p>
         <script>
            (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
                  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
               m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
            })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

            ga('create', 'UA-78549539-1', 'auto');
            ga('send', 'pageview');

         </script>
		 <table class="imgtable">
			 <tr><td>
					 <a href="https://jonghwanmun.github.io"><img src="imgs/jonghwanmun.jpg" alt="" width="200px" height="200px" /></a>&nbsp;</td>
				 <td align="left"><p><b><font size="6"> Jonghwan Mun </font></b>
					 <br /> <br />
					 Ph.D. in Computer Science and Engineering <br />
					 Research Scientist at Kakao Brain <br /><br />
					 Email : <a href="mailto:choco1916@postech.ac.kr">jason.mun [at] kakaobrain [dot] com </a> <br />
					 <a href="https://scholar.google.co.kr/citations?user=_UhbCBEAAAAJ&hl=ko">google scholar</a>
					 </p>
				 </td></tr>
		 </table>

         <!--
         <h1>News</h1>
         <ul>
            <li><p>[Feb 2020] One paper is accepted to CVPR 2020.
            <li><p>[Nov 2019] One paper is accepted to AAAI 2020.
            <li><p>[Apr 2019] Our paper entiled "Streamlined Dense Video Captioning" is now available on arXiv. This work is done during my internship program at Sanp Research.
                  <a class="news" target="_blank" href="http://arxiv.org/abs/1904.03870">[arXiv preprint]</a>
            </li>
            <li><p>[Apr 2019] We release the dataset and code for the paper "Transfer Learning via Unsupervised Task Discovery for Visual Question Answering" 
                  and the paper is available on arXiv.
                  <a class="news" target="_blank" href="https://arxiv.org/abs/1810.02358">[arXiv preprint]</a>
                  <a class="news" target="_blank" href="https://github.com/HyeonwooNoh/vqa_task_discovery">[code]</a>
            </li>
            <li><p>[Feb 2019] Two papers "Streamlined Dense Video Captioning" <font color=red>(oral)</font> and "Transfer Learning via Unsupervised Task Discovery for Visual Question Answering" are accepted to CVPR 2019.
            </li>
            <li><p>[Sep 2018] Our paper "Learning to Specialize with Knowledge Distillation for Visual Question Answering" is accepted to NeurIPS 2018.
            </li>
            <li><p>[Jun 2018] I am working at Snap Research in Venice as a research intern.
            </li>
            Our paper <strong>"Regularizing Deep Neural Networks by Noise: Its Interpretation and Optimization"</strong> is accepted to <strong>NIPS 2017</strong>.
<a class="news" target="_blank" href="https://arxiv.org/abs/1710.05179">[paper]</a><a class="news" target="_blank" href="./papers/noh2017regularizing/noh2017regularizing_poster.pdf">[poster]</a><a class="news" target="_blank" href="https://youtu.be/QA8K4zQmLc4">[video]</a/>
            <li><p>[Dec 2016] Our paper about new dataset of video question answering, which is called as MarioQA, is now available at <b><a href="http://arxiv.org/abs/1612.01669">arXiv</a></b> </p>
            </li>
            <li><p>[Nov 2016] Our paper about image captioning has been accepted to AAAI 2017 </p>
            </li>
         </ul>
         -->


         <!-- Education Section -->
         <h1>Education</h1>
         <ul>
            <li><p><b>M.S. &amp; Ph.D. integrated course</b>, <a href="http://cv.postech.ac.kr">Computer Vision Lab</a>, <a href="http://postech.ac.kr">POSTECH</a> (Mar. 2014 &ndash; Feb. 2020)</p>
            </li>
			<ul>
				<li>Advisor : <a href="http://cv.postech.ac.kr/~bhhan">Prof. Bohyung Han</a>
					and <a href="http://cv.postech.ac.kr/~mcho">Prof. Minsu Cho</a></li>
			</ul>
            <li><p><b>B.S</b>. <a href="http://ece.unist.ac.kr">School of Electrical and Computer Engineering</a>, <a href="http://unist.ac.kr">UNIST</a> (Mar. 2010 &ndash; Feb. 2014)</p>
            </li>
         </ul>

         <!-- Work Experiences Section -->
         <h1>Work Experiences</h1>
         <ul>
            <li><p><b>Research Scientist</b>, Kakao Brain (Mar. 2020 &ndash; Current)</p>
            </li>
            <li><p><b>Research Intern</b>, Snap Inc., Venice, California, USA (Jun. 2018 &ndash; Aug. 2018)</p>
            </li>
         </ul>


         <!-- Publication Section -->
         <h1>Publications</h1>

	  <!-- SoF -->
	 <table class="imgtable"><tr><td>
			 <img src="imgs/SoF.png" alt="" width="280px" height="110px" />&nbsp;</td>
			 <td align="left"><p><b><a href="https://jonghwanmun.github.io/">
					    Stop or Forward: Dynamic Layer Skipping for Efficient Action Recognition
					 </a></b> <br />
				 	 <a href="https://www.linkedin.com/in/jonghyeon-seon-4958918a/">Jonghyeon Seon</a>, <a href="https://jd730.github.io/">Jaedong Hwang</a>, <u><b>Jonghwan Mun</b></u>, Bohyung Han. <br />
					 In WACV 2023 <br />
					 <a class="news" target="_blank" href="https://jonghwanmun.github.io/">[arXiv preprint]</a>
					 <a class="news" target="_blank" href="https://github.com/sunutf/SoF-Net">[code]</a></p>
				 	 <a class="news" target="_blank" href="https://jd730.github.io/files/seon2023stop_poster.pdf">[poster]</a></p>
				 	
	</td></tr></table>
	<p><br /></p>
	      
         <!-- BaSSL -->
		 <table class="imgtable"><tr><td>
					 <img src="imgs/BaSSL.png" alt="" width="280px" height="110px" />&nbsp;</td>
				 <td align="left"><p><b><a href="http://cvlab.postech.ac.kr/~jonghwan">
							 BaSSL: Boundary-aware Self-supervised Learning for Video Scene Segmentation
						 </a></b> <br />
						 <u><b>Jonghwan Mun</b></u>, Minchul Shin, Gunsoo Han, Sangho Lee, Seongsu Ha, <a href="http://www.joonseok.net/home.html">Joonseok Lee</a>, <a href="https://kimeunsol.github.io/">Eun-Sol Kim</a> <br />
						 In ACCV 2022 <br />
						 <a class="news" target="_blank" href="https://arxiv.org/pdf/2201.05277.pdf">[arXiv preprint]</a>
						 <a class="news" target="_blank" href="https://github.com/kakaobrain/bassl">[code]</a></p>
		</td></tr></table>
		<p><br /></p>

         <!-- MSTR -->
         <table class="imgtable"><tr><td>
                  <img src="imgs/MSTR.png" alt="" width="280px" height="110px" />&nbsp;</td>
               <td align="left"><p><b><a href="http://cvlab.postech.ac.kr/~jonghwan">
						MSTR: Multi-Scale Transformer for End-to-End Human-Object Interaction Detection
                  </a></b> <br />
				  <a href="https://meliketoy.github.io/">Bumsoo Kim</a>, <u><b>Jonghwan Mun</b></u>, Kyoung-Woon On, Minchul Shin, Junhyun Lee, <a href="https://kimeunsol.github.io/">Eun-Sol Kim</a> <br />
                  In CVPR 2022 <br />
                  <a class="news" target="_blank" href="https://openaccess.thecvf.com/content/CVPR2022/papers/Kim_MSTR_Multi-Scale_Transformer_for_End-to-End_Human-Object_Interaction_Detection_CVPR_2022_paper.pdf">[paper]</a>
				  <a class="news" target="_blank" href="https://www.youtube.com/watch?v=NuMGPhAC8ZY&feature=youtu.be">[video]</a></p>
         </td></tr></table>
         <p><br /></p>

         <!-- LGI-->
         <table class="imgtable"><tr><td>
                  <img src="imgs/LGI.png" alt="" width="280px" height="110px" />&nbsp;</td>
               <td align="left"><p><b><a href="http://cvlab.postech.ac.kr/~jonghwan">
                        Local-Global Video-Text Interactions for Temporal Grounding
                  </a></b> <br />
				  <u><b>Jonghwan Mun</b></u>, Minsu Cho, Bohyung Han <br />
                  In CVPR 2020 <br />
                  <a class="news" target="_blank" href="https://arxiv.org/abs/2004.07514">[arXiv preprint]</a>
				  <a class="news" target="_blank" href="https://github.com/JonghwanMun/LGI4temporalgrounding">[code]</a></p>
         </td></tr></table>
         <p><br /></p>

         <!-- KDAS -->
         <table class="imgtable"><tr><td>
                  <img src="imgs/KDAS.png" alt="" width="280px" height="110px" />&nbsp;</td>
               <td align="left"><p><b><a href="http://cvlab.postech.ac.kr/~jonghwan">
                        Towards Oracle Knowledge Distillation with Neural Architecture Search
                  </a></b> <br />
				  Minsoo Kang*, <u><b>Jonghwan Mun*</b></u>, Bohyung Han (* equal contribution) <br />
                  In AAAI 2020 <br />
				  <a class="news" target="_blank" href="https://arxiv.org/abs/1911.13019">[arXiv preprint]</a></p>
         </td></tr></table>
         <p><br /></p>

         <!-- Streamlined Dense Video Captioning-->
         <table class="imgtable"><tr><td>
                  <img src="imgs/SDVC.png" alt="" width="280px" height="110px" />&nbsp;</td>
               <td align="left"><p><b><a href="http://cvlab.postech.ac.kr/~jonghwan">Streamlined Dense Video Captioning</a></b> <br />
				   <u><b>Jonghwan Mun</b></u>, Linjie Yang, Zhou Ren, Ning Xu, Bohyung Han<br />
                  In CVPR 2019, <font color=red size=+0.5><b>Oral presentation</b></font> (Acceptance = 288/5160~5.6%)<br />
				  <a class="news" target="_blank" href="http://arxiv.org/abs/1904.03870">[arXiv preprint]</a></p>
         </td></tr></table>
         <p><br /></p>


         <!-- Transfer Learning via Unsupervised Task Discovery for Visual Question Answering -->
         <table class="imgtable"><tr><td>
                  <img src="imgs/vqa_transfer_figure.png" alt="" width="280px" height="110px" />&nbsp;</td>
               <td align="left"><p><b><a href="http://cvlab.postech.ac.kr/~jonghwan"> Transfer Learning via Unsupervised Task Discovery for Visual Question Answering </a></b> <br />
				   Hyeonwoo Noh, Taehoon Kim, <u><b>Jonghwan Mun</b></u>, Bohyung Han<br />
                  In CVPR 2019 <br />
                  <a class="news" target="_blank" href="https://arxiv.org/abs/1810.02358">[arXiv preprint]</a>
				  <a class="news" target="_blank" href="https://github.com/HyeonwooNoh/vqa_task_discovery">[code]</a></p>
         </td></tr></table>
         <p><br /></p>

         <!-- Knowledge Distillation with Multiple Choice Learning-->
         <table class="imgtable"><tr><td>
                  <img src="imgs/MCL-KD.png" alt="" width="280px" height="110px" />&nbsp;</td>
               <td align="left"><p><b><a href="http://cvlab.postech.ac.kr/~jonghwan">Learning to Specialize with Knowledge Distillation for Visual Question Answering</a></b> <br />
				   <u><b>Jonghwan Mun</b></u>, Kimin Lee, Jinwoo Shin, Bohyung Han<br />
                  In NuerIPS 2018 <br />
				  <a target="_blank" href="https://papers.nips.cc/paper/8031-learning-to-specialize-with-knowledge-distillation-for-visual-question-answering.pdf">[paper]</a></p>
         </td></tr></table>
         <p><br /></p>


         <!-- Regularization by noise-->
         <table class="imgtable"><tr><td>
                  <img src="imgs/iwsgd.png" alt="" width="280px" height="130px" />&nbsp;</td>
               <td align="left"><p><b><a href="http://cvlab.postech.ac.kr/~jonghwan">Regularizing Deep Neural Networks by Noise: Its Interpretation and Optimization</a></b> <br />
				   Hyeonwoo Noh, Tackgeun You, <u><b>Jonghwan Mun</b></u>, Bohyung Han<br />
                  In NIPS 2017 <br />
                  <a target="_blank" href="https://arxiv.org/abs/1710.05179">[arXiv preprint]</a>
                  <a class="news" target="_blank" href="https://papers.nips.cc/paper/7096-regularizing-deep-neural-networks-by-noise-its-interpretation-and-optimization">[paper]</a>
                  <a class="news" target="_blank" href="papers/noh2017regularizing/noh2017regularizing_poster.pdf">[poster]</a>
				  <a class="news" target="_blank" href="https://youtu.be/QA8K4zQmLc4">[video]</a/></p>
         </td></tr></table>
         <p><br /></p>


         <!-- MarioQA -->
         <table class="imgtable"><tr><td>
                  <img src="imgs/qa_construction.jpg" alt="" width="280px" height="130px" />&nbsp;</td>
               <td align="left"><p><b><a href="http://arxiv.org/abs/1612.01669">MarioQA: Answering Questions by Watching Gameplay Video</a></b> <br />
				   <u><b>Jonghwan Mun*</b></u>, Paul Hongsuck Seo*, Ilchae Jung, Bohyung Han (* equal contribution) <br />
                  In ICCV 2017 <br />
                  [<a href="http://arxiv.org/abs/1612.01669">arXiv preprint</a>]
                  [<a href="http://openaccess.thecvf.com/content_iccv_2017/html/Mun_MarioQA_Answering_Questions_ICCV_2017_paper.html">paper</a>]
                  [<a href="http://cvlab.postech.ac.kr/research/MarioQA">project page</a>] </p>
         </td></tr></table>
         <p><br /></p>


         <!-- Text-guided -->
         <table class="imgtable"><tr><td>
                  <img src="imgs/textATT_architecture.png" alt="" width="280px" height="110px" />&nbsp;</td>
               <td align="left"><p><b><a href="http://arxiv.org/abs/1612.03557">Text-guided Attention Model for Image Captioning</a></b> <br />
				   <u><b>Jonghwan Mun</b></u>, Minsu Cho, Bohyung Han <br />
                  In AAAI 2017 <br />
                  [<a href="http://arxiv.org/abs/1612.03557">arXiv preprint</a>] 
                  [<a href="https://aaai.org/ocs/index.php/AAAI/AAAI17/paper/view/14888">paper</a>] 
                  [<a href="http://github.com/JonghwanMun/TextguidedATT">code</a>]
                  [<a href="http://cvlab.postech.ac.kr/research/text_att">project page</a>] </p>
         </td></tr></table>
         <p><br /></p>


         <!-- Awards and Honors Section -->
         <h1>Awards and Honors</h1>
         <ul>
			 <li><p><b><a href="https://value-benchmark.github.io/leaderboard.html">VALUE Challenge</a></b>, <b><u>1st place</u></b> at VALUE and QA phase, ICCV 2021'CLVL Workshop</p>
            </li>
            <li><p><b>Naver Ph.D. Fellowship</b>, 2018</p>
            </li>
            <li><p><b>National Science and Technology Undergraduate Scholarship</b>, Korea Student Aid Foundation, 2010 â€“ 2013</p>
            </li>
         </ul>


         <!-- Projects Section -->
		 <!--
         <h1>Projects</h1>
         <ul>
            <li><p><b>Video Question Answering with Temporal Reasoning</b> <br />
            Software R&amp;D Center, Samsung Electronics Co., South Korea <br />
            (Aug 2016 - Dec 2016)</p>
            </li>
         </ul>
         <ul>
            <li><p><b>Development of Image Caption Generation Algorithm</b> <br />
            DMC R&amp;D Center, Samsung Electronics Co., South Korea <br />
            (Aug 2015 - Jul 2016)</p>
            </li>
         </ul>
		 -->
      </div>
   </body>
</html>
